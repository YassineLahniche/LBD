{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "011d96ec-9499-4e1b-a9ce-9eaa3dc8cac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Numerical computations\n",
    "import gym  # RL environment library\n",
    "from gym import spaces  # Used to define state and action spaces\n",
    "import matplotlib.pyplot as plt  # Used for visualization and plotting\n",
    "import pandas as pd  # Used for handling datasets (e.g., weather data)\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20d5abcc-86ea-4cc9-b72c-b92eb9a70078",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Experience replay buffer to store and sample transitions.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add experience to buffer\"\"\"\n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Randomly sample batch of experiences\"\"\"\n",
    "        # Make sure we don't sample more than buffer size\n",
    "        batch_size = min(batch_size, len(self.buffer))\n",
    "        \n",
    "        # Sample random indices\n",
    "        indices = random.sample(range(len(self.buffer)), batch_size)\n",
    "        \n",
    "        # Get samples\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        for i in indices:\n",
    "            s, a, r, s_, d = self.buffer[i]\n",
    "            states.append(s)\n",
    "            actions.append(a)\n",
    "            rewards.append(r)\n",
    "            next_states.append(s_)\n",
    "            dones.append(d)\n",
    "        \n",
    "        return (\n",
    "            np.array(states), \n",
    "            np.array(actions), \n",
    "            np.array(rewards).reshape(-1, 1), \n",
    "            np.array(next_states),\n",
    "            np.array(dones).reshape(-1, 1)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return current buffer size\"\"\"\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def is_ready(self, batch_size):\n",
    "        \"\"\"Check if buffer has enough experiences\"\"\"\n",
    "        return len(self) >= batch_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8ff3790-5ddb-4578-8833-75fc7ca0cc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridEnergyEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(HybridEnergyEnv, self).__init__()\n",
    "        \n",
    "        # Define state and action sizes\n",
    "        self.state_size = 4  # [P_solar, P_wind, Energy demand, Grid price]\n",
    "        self.action_size = 3  # [N_pv, N_wt, P_grid]\n",
    "        \n",
    "        # Action space bounds\n",
    "        self.action_low = np.array([0, 0, 0])  # Min panels, min turbines, min grid power\n",
    "        self.action_high = np.array([300, 50, 1000])  # Max panels, max turbines, max grid power\n",
    "        \n",
    "        # Define action and observation spaces\n",
    "        self.action_space = spaces.Box(\n",
    "            low=self.action_low,\n",
    "            high=self.action_high,\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0, 0, 0, 0]),  # Min values for state\n",
    "            high=np.array([1000, 1000, 1000, 1]),  # Max values for state\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Emission factors (gCO2/kWh)\n",
    "        self.EF_PV = 50  # Emission factor per solar panel\n",
    "        self.EF_WT = 10  # Emission factor per wind turbine\n",
    "        self.EF_grid = 300  # Emission factor for grid power\n",
    "        \n",
    "        # Initialize state\n",
    "        self.current_state = None\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to an initial state.\"\"\"\n",
    "        self.current_state = np.array([0, 0, 1000, 0.1])  # Example initial state\n",
    "        return self.current_state\n",
    "\n",
    "    def estimate_solar_panel_cost(self, wattage, sunlight_hours_per_day, lifetime_years):\n",
    "        \"\"\"Estimate the cost per kWh of a solar panel over its lifetime.\"\"\"\n",
    "        if wattage==0: return 0\n",
    "        else:\n",
    "            cost_per_watt = 2.75  # Cost per watt ($/W)\n",
    "            maintenance_cost_per_year = 25  # Maintenance cost ($/year)\n",
    "            capital_cost = wattage * cost_per_watt  # Initial investment\n",
    "            total_maintenance_cost = maintenance_cost_per_year * lifetime_years\n",
    "            total_cost = capital_cost + total_maintenance_cost\n",
    "            lifetime_energy_production_kwh = (wattage * sunlight_hours_per_day * 365 * lifetime_years) / 1000\n",
    "            return total_cost / lifetime_energy_production_kwh  # Cost per kWh\n",
    "\n",
    "    def estimate_wind_turbine_cost(self, rated_power_kw, capacity_factor, lifetime_years):\n",
    "        \"\"\"Estimate the cost per kWh of a wind turbine over its lifetime.\"\"\"\n",
    "        if rated_power_kw==0: return 0\n",
    "        else:\n",
    "            cost_per_kw = 1500  # Cost per kW ($/kW)\n",
    "            maintenance_cost_per_year = 50  # Maintaenance cost ($/year)\n",
    "            capital_cost = rated_power_kw * cost_per_kw  # Initial investment\n",
    "            total_maintenance_cost = maintenance_cost_per_year * lifetime_years\n",
    "            total_cost = capital_cost + total_maintenance_cost\n",
    "            lifetime_energy_production_kwh = rated_power_kw * capacity_factor * 24 * 365 * lifetime_years\n",
    "            return total_cost / lifetime_energy_production_kwh  # Cost per kWh\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Execute one time step in the environment.\"\"\"\n",
    "        # Extract action components\n",
    "        N_pv = int(action[0])  # Number of PV panels (integer)\n",
    "        N_wt = int(action[1])  # Number of wind turbines (integer)\n",
    "        P_grid_action = action[2]  # Grid power (continuous)\n",
    "        \n",
    "        # Extract current state\n",
    "        P_solar, P_wind, energy_demand, grid_price = self.current_state\n",
    "        \n",
    "        # Compute energy generated (with efficiency)\n",
    "        P_pv = N_pv * P_solar * 0.2  # 20% efficiency for solar\n",
    "        P_wt = N_wt * P_wind * 0.5  # 50% efficiency for wind\n",
    "        total_renewable_energy = P_pv + P_wt\n",
    "        \n",
    "        # Compute energy deficit and grid usage\n",
    "        energy_deficit = max(0, energy_demand - total_renewable_energy)\n",
    "        grid_power_used = min(P_grid_action, energy_deficit)\n",
    "        \n",
    "        # Compute costs\n",
    "        solar_cost = self.estimate_solar_panel_cost(N_pv * 300, 5, 25)  # 300W panels, 5h sunlight/day\n",
    "        wind_cost = self.estimate_wind_turbine_cost(N_wt * 10000, 0.35, 25)  # 10kW turbines, 35% capacity\n",
    "        grid_cost = grid_power_used * grid_price\n",
    "        total_cost = solar_cost + wind_cost + grid_cost\n",
    "        \n",
    "        # Compute carbon footprint\n",
    "        carbon_footprint = (\n",
    "            (self.EF_PV * N_pv) + \n",
    "            (self.EF_WT * N_wt) + \n",
    "            (self.EF_grid * grid_power_used)\n",
    "        )\n",
    "        \n",
    "        # Compute reward (negative cost and emissions)\n",
    "        reward = - (total_cost + carbon_footprint)\n",
    "        \n",
    "        # Update state with random fluctuations\n",
    "        next_P_solar = np.clip(P_solar + np.random.uniform(-50, 50), 0, 1200)\n",
    "        next_P_wind = np.clip(P_wind + np.random.uniform(-2, 2), 0, 25)\n",
    "        next_state = np.array([\n",
    "            next_P_solar,\n",
    "            next_P_wind,\n",
    "            energy_demand,  # Demand remains fixed\n",
    "            grid_price  # Grid price remains fixed\n",
    "        ])\n",
    "        \n",
    "        self.current_state = next_state\n",
    "        done = False\n",
    "        \n",
    "        return next_state, reward, done, {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b5dba0b-01e3-4c4c-9378-5b97a9a9971d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yassine Lahniche\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\spaces\\box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward: -8399696.777274515\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 117. MiB for an array with shape (15366351,) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 120\u001b[0m\n\u001b[0;32m    117\u001b[0m agent \u001b[38;5;241m=\u001b[39m QAgent(env)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# Train Agent\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m q_learning_rewards \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# Test Agent vs. Naive Strategy\u001b[39;00m\n\u001b[0;32m    123\u001b[0m agent_reward \u001b[38;5;241m=\u001b[39m test_agent(agent, episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 71\u001b[0m, in \u001b[0;36mQAgent.train\u001b[1;34m(self, episodes)\u001b[0m\n\u001b[0;32m     69\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchoose_action(state)\n\u001b[0;32m     70\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m---> 71\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_q_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     73\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[4], line 55\u001b[0m, in \u001b[0;36mQAgent.update_q_table\u001b[1;34m(self, state, action, reward, next_state)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_table[state] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space))\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_state \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_table:\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_table[next_state] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space))\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Q-learning update rule\u001b[39;00m\n\u001b[0;32m     58\u001b[0m best_next_q \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_table[next_state])  \u001b[38;5;66;03m# Best Q-value for next state\u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 117. MiB for an array with shape (15366351,) and data type float64"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class QAgent:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.95, epsilon=1.0, epsilon_decay=0.99, min_epsilon=0.01):\n",
    "        self.env = env\n",
    "        \n",
    "        # Define discretized actions\n",
    "        self.n_pv_values = np.arange(0, 301)  # 0 to 100 panels \n",
    "        self.n_wt_values = np.arange(0, 51)   # 0 to 50 turbines \n",
    "        self.p_grid_values = np.arange(0, 1001)  # 0 to 1000 grid power \n",
    "\n",
    "        # Convert to tuples for indexing\n",
    "        self.action_space = [(pv, wt, grid) for pv in self.n_pv_values for wt in self.n_wt_values for grid in self.p_grid_values]\n",
    "\n",
    "        # Q-table: map (state -> action values)\n",
    "        self.q_table = {}\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.alpha = alpha      # Learning rate\n",
    "        self.gamma = gamma      # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "\n",
    "    def discretize_state(self, state):\n",
    "        \"\"\"Convert continuous state to a discretized key.\"\"\"\n",
    "        return tuple(np.round(state, decimals=1))  # Round to 1 decimal for a manageable table size\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Epsilon-greedy policy for action selection.\"\"\"\n",
    "        state = self.discretize_state(state)\n",
    "        \n",
    "        # Initialize Q-values if state is new\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = np.zeros(len(self.action_space))\n",
    "        \n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.choice(self.action_space)  # Explore\n",
    "        else:\n",
    "            best_action_idx = np.argmax(self.q_table[state])  # Exploit\n",
    "            return self.action_space[best_action_idx]\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        \"\"\"Update Q-table using the Q-learning formula.\"\"\"\n",
    "        state = self.discretize_state(state)\n",
    "        next_state = self.discretize_state(next_state)\n",
    "        action_idx = self.action_space.index(action)\n",
    "\n",
    "        # Initialize Q-values if states are new\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = np.zeros(len(self.action_space))\n",
    "        if next_state not in self.q_table:\n",
    "            self.q_table[next_state] = np.zeros(len(self.action_space))\n",
    "\n",
    "        # Q-learning update rule\n",
    "        best_next_q = np.max(self.q_table[next_state])  # Best Q-value for next state\n",
    "        self.q_table[state][action_idx] += self.alpha * (reward + self.gamma * best_next_q - self.q_table[state][action_idx])\n",
    "\n",
    "    def train(self, episodes=1000):\n",
    "        rewards = []\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            i = 0\n",
    "            while i<100:\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                self.update_q_table(state, action, reward, next_state)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                i+=1\n",
    "\n",
    "            self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)  # Decay epsilon\n",
    "            rewards.append(total_reward)\n",
    "            if episode % 10 == 0:\n",
    "                print(f\"Episode {episode}, Reward: {total_reward}\")\n",
    "\n",
    "        return rewards\n",
    "\n",
    "def test_agent(agent, episodes=20):\n",
    "    \"\"\"Test the trained agent and return average reward.\"\"\"\n",
    "    total_rewards = []\n",
    "    for _ in range(episodes):\n",
    "        state = agent.env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        i=0\n",
    "        while i<100:\n",
    "            action = agent.choose_action(state)  # Best known action\n",
    "            state, reward, done, _ = agent.env.step(action)\n",
    "            total_reward += reward\n",
    "            i+=1\n",
    "        total_rewards.append(total_reward)\n",
    "    return np.mean(total_rewards)\n",
    "\n",
    "def naive_strategy(env, episodes=100):\n",
    "    \"\"\"Baseline strategy: always use a fixed number of panels/turbines/grid power.\"\"\"\n",
    "    total_rewards = []\n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        i=0\n",
    "        while i<100:\n",
    "            action = (50, 20, 500)  # Fixed naive strategy\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            i+=1\n",
    "        total_rewards.append(total_reward)\n",
    "    return np.mean(total_rewards)\n",
    "\n",
    "# Initialize Environment and Agent\n",
    "env = HybridEnergyEnv()\n",
    "agent = QAgent(env)\n",
    "\n",
    "# Train Agent\n",
    "q_learning_rewards = agent.train(episodes=100)\n",
    "\n",
    "# Test Agent vs. Naive Strategy\n",
    "agent_reward = test_agent(agent, episodes=100)\n",
    "naive_reward = naive_strategy(env, episodes=100)\n",
    "\n",
    "print(f\"Q-learning Agent Average Reward: {agent_reward}\")\n",
    "print(f\"Naive Strategy Average Reward: {naive_reward}\")\n",
    "\n",
    "# Plot Training Rewards\n",
    "plt.plot(q_learning_rewards)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Q-learning Training Progress\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce3c1d6-3ec2-4d69-bc98-d8fc5debd982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_actions(agent, states):\n",
    "    \"\"\"Get the best actions for multiple states using the trained Q-table.\"\"\"\n",
    "    best_actions = {}\n",
    "    \n",
    "    for state in states:\n",
    "        state_key = agent.discretize_state(state)  # Convert to discrete form\n",
    "        if state_key in agent.q_table:\n",
    "            best_action = max(agent.q_table[state_key], key=agent.q_table[state_key].get)\n",
    "        else:\n",
    "            best_action = random.choice(agent.action_space)  # Fallback to random action\n",
    "        \n",
    "        best_actions[tuple(state)] = best_action  # Store result\n",
    "    \n",
    "    return best_actions\n",
    "\n",
    "# Example states\n",
    "test_states = [\n",
    "    [500, 10, 800, 0.2],   # Moderate solar & wind\n",
    "    [700, 5, 1200, 0.4],   # High demand, low wind\n",
    "    [300, 15, 600, 0.1],   # Low solar, high wind\n",
    "    [1000, 20, 500, 0.3],  # High solar & wind\n",
    "    [200, 2, 1000, 0.5]    # Low energy generation, high grid price\n",
    "]\n",
    "\n",
    "# Get best actions\n",
    "best_actions = get_best_actions(agent, test_states)\n",
    "\n",
    "# Print results\n",
    "for state, action in best_actions.items():\n",
    "    print(f\"State: {state} -> Best Action: {action}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f87b7f1-fa8d-4e43-afad-e48444840f45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
