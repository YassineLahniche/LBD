import numpy as np
import random

class QAgent:
    def __init__(self, env, alpha=0.02, gamma=0.98, epsilon=1.0, epsilon_decay=0.99, min_epsilon=0.08, max_energy=1000):
        self.env = env
        self.max_energy = max_energy  # Maximum total energy allowed

        # Reduce action space dimensionality
        self.n_pv_values = np.arange(0, 301, 1)  # 0 to 300 panels in steps of 10
        self.n_wt_values = np.arange(0, 51, 1)    # 0 to 50 turbines in steps of 5
        self.p_grid_values = np.arange(0, 1001, 50)  # 0 to 1000 grid power in steps of 50

        # Use a sparse representation instead of generating all combinations
        self.action_space_pv = list(self.n_pv_values)
        self.action_space_wt = list(self.n_wt_values)
        self.action_space_grid = list(self.p_grid_values)
        
        # Store action space sizes for faster indexing
        self.pv_size = len(self.action_space_pv)
        self.wt_size = len(self.action_space_wt)
        self.grid_size = len(self.action_space_grid)
        self.action_count = self.pv_size * self.wt_size * self.grid_size

        # Use a dictionary for sparse Q-table representation
        self.q_table = {}

        # Hyperparameters
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.min_epsilon = min_epsilon
        
        # Track visited states for memory optimization
        self.visited_states = set()

    def discretize_state(self, state):
        """Convert continuous state to a discretized key with coarser granularity."""
        return tuple(np.round(state, decimals=1))

    def action_to_idx(self, action):
        """Convert action tuple to flat index."""
        pv_idx = self.action_space_pv.index(action[0])
        wt_idx = self.action_space_wt.index(action[1])
        grid_idx = self.action_space_grid.index(action[2])
        return pv_idx * (self.wt_size * self.grid_size) + wt_idx * self.grid_size + grid_idx

    def idx_to_action(self, idx):
        """Convert flat index to action tuple."""
        grid_idx = idx % self.grid_size
        temp = idx // self.grid_size
        wt_idx = temp % self.wt_size
        pv_idx = temp // self.wt_size
        
        return (self.action_space_pv[pv_idx], 
                self.action_space_wt[wt_idx], 
                self.action_space_grid[grid_idx])

    def estimate_total_energy(self, action, state):
        """
        Estimate the total energy generated by the given action.
        This is a simplified model - replace with actual calculation based on your environment.
        
        Parameters:
        - action: Tuple of (pv_count, wt_count, grid_power)
        - state: Current state, which may contain weather/solar data
        
        Returns:
        - estimated_energy: Total energy from all sources
        """
        pv_count, wt_count, grid_power = action
        
 
        pv_power = state[0]  # Default if not in state
        wind_power = state[1]        # Default if not in state
        
        # Simplified energy calculations
        pv_energy = pv_count * pv_power   
        wt_energy = wt_count * wind_power      
        
        total_energy = pv_energy + wt_energy + grid_power
        return total_energy

    def get_valid_action(self, action, state):
        """
        Adjust the action to ensure total energy doesn't exceed the maximum limit.
        If energy would exceed the limit, reduce grid power to stay within bounds.
        """
        pv_count, wt_count, grid_power = action
        total_energy = self.estimate_total_energy(action, state)
        
        if total_energy <= self.max_energy:
            return action  # Action is already valid
        
        # We need to reduce energy - prioritize reducing grid power first
        excess_energy = total_energy - self.max_energy
        adjusted_grid = max(0, grid_power - excess_energy)
        
        # Find the closest valid grid power value in our discretized space
        adjusted_grid = min(self.action_space_grid, key=lambda x: abs(x - adjusted_grid))
        
        # If grid reduction isn't enough, we'd need more complex adjustments to PV/WT
        # but for simplicity, we'll just use this adjustment
        adjusted_action = (pv_count, wt_count, adjusted_grid)
        
        # Verify the adjustment worked
        if self.estimate_total_energy(adjusted_action, state) > self.max_energy:
            # If still over limit, find the most conservative valid action
            for g in self.action_space_grid:
                test_action = (pv_count, wt_count, g)
                if self.estimate_total_energy(test_action, state) <= self.max_energy:
                    return test_action
            
            # If no valid action found with current PV/WT, reduce them
            return (0, 0, min(self.action_space_grid, key=lambda x: x if x <= self.max_energy else float('inf')))
        
        return adjusted_action

    def choose_action(self, state):
        """Epsilon-greedy policy for action selection with energy constraint."""
        state = self.discretize_state(state)
        
        # Explore: Randomly sample from action space components
        if np.random.rand() < self.epsilon:
            pv = random.choice(self.action_space_pv)
            wt = random.choice(self.action_space_wt)
            grid = random.choice(self.action_space_grid)
            action = (pv, wt, grid)
            return self.get_valid_action(action, state)  # Ensure action meets energy constraint
        
        # Exploit: Use the best known action
        if state not in self.q_table:
            # Initialize on-demand with a single random action
            self.q_table[state] = {0: 0.0}
            action = self.idx_to_action(0)
            return self.get_valid_action(action, state)  # Ensure action meets energy constraint
        
        # Find best action index
        best_idx = max(self.q_table[state], key=self.q_table[state].get)
        action = self.idx_to_action(best_idx)
        return self.get_valid_action(action, state)  # Ensure action meets energy constraint

    def update_q_table(self, state, action, reward, next_state):
        """Update Q-table using the Q-learning formula with sparse representation."""
        state = self.discretize_state(state)
        next_state = self.discretize_state(next_state)
        
        # Convert action to index - handle non-discretized values
        try:
            pv = min(self.action_space_pv, key=lambda x: abs(x - action[0]))
            wt = min(self.action_space_wt, key=lambda x: abs(x - action[1]))
            grid = min(self.action_space_grid, key=lambda x: abs(x - action[2]))
            mapped_action = (pv, wt, grid)
            action_idx = self.action_to_idx(mapped_action)
        except ValueError:
            # Fallback if conversion fails
            action_idx = 0
        
        # Initialize state entry if new
        if state not in self.q_table:
            self.q_table[state] = {}
        
        # Initialize action value if new
        if action_idx not in self.q_table[state]:
            self.q_table[state][action_idx] = 0.0
            
        # Add to visited states
        self.visited_states.add(state)
            
        # Find maximum Q-value for next state
        if next_state in self.q_table and self.q_table[next_state]:
            best_next_q = max(self.q_table[next_state].values())
        else:
            best_next_q = 0.0
            
        # Update rule
        self.q_table[state][action_idx] += self.alpha * (
            reward + self.gamma * best_next_q - self.q_table[state][action_idx]
        )
        
        # Remove near-zero Q-values to save memory
        if abs(self.q_table[state][action_idx]) < 1e-6:
            del self.q_table[state][action_idx]
            # Remove empty state entries
            if not self.q_table[state]:
                del self.q_table[state]

    def train(self, episodes=1000, max_steps=15000):
        rewards = []
        energy_violations = 0
        
        for episode in range(episodes):
            state = self.env.reset()
            total_reward = 0
            episode_violations = 0
            
            for step in range(max_steps):
                action = self.choose_action(state)
                
                # Check if original action would violate energy constraint
                total_energy = self.estimate_total_energy(action, state)
                if total_energy > self.max_energy:
                    episode_violations += 1
                
                next_state, reward, done, _ = self.env.step(action)
                
                # Additional penalty for actions that would exceed energy limit
                if total_energy > self.max_energy:
                    energy_penalty = -abs(total_energy - self.max_energy) * 0.1  # Scale penalty by excess
                    reward += energy_penalty
                
                self.update_q_table(state, action, reward, next_state)
                state = next_state
                total_reward += reward
                
                if done:
                    break
                    
            # Decay epsilon
            self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)
            rewards.append(total_reward)
            energy_violations += episode_violations
            
            # Print progress less frequently
            if episode % 20 == 0:
                print(f"Episode {episode}, Reward: {total_reward:.2f}, Violations: {episode_violations}")
                
        print(f"Training complete. Total energy violations: {energy_violations}")
        print(f"Final Q-table size: {len(self.q_table)} states")
        return rewards

