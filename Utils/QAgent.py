import numpy as np
import random
import matplotlib.pyplot as plt

class EnhancedQAgent:
    def __init__(self, env, alpha=0.1, gamma=0.98, epsilon=1.0, epsilon_decay=0.995, min_epsilon=0.05, max_energy=200):
        self.env = env
        self.max_energy = max_energy
        
        # Reduce action space dimensionality for better learning
        self.n_pv_values = np.arange(0, 301, 10)  # 0 to 300 panels in steps of 10
        self.n_wt_values = np.arange(0, 51, 5)    # 0 to 50 turbines in steps of 5
        self.p_grid_values = np.arange(0, 201, 20)  # 0 to 200 grid power in steps of 20
        
        self.action_space_pv = list(self.n_pv_values)
        self.action_space_wt = list(self.n_wt_values)
        self.action_space_grid = list(self.p_grid_values)
        
        self.pv_size = len(self.action_space_pv)
        self.wt_size = len(self.action_space_wt)
        self.grid_size = len(self.action_space_grid)
        self.action_count = self.pv_size * self.wt_size * self.grid_size
        
        # Use a dictionary for sparse Q-table representation
        self.q_table = {}
        
        # Target network implementation
        self.target_q_table = {}
        self.update_target_counter = 0
        self.target_update_frequency = 10  # Update target network every 10 episodes
        
        # Hyperparameters - improved values
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.min_epsilon = min_epsilon
        
        # Track visited states for memory optimization
        self.visited_states = set()
        
        # Track metrics separately
        self.cost_history = []
        self.co2_history = []
        self.energy_violation_history = []
        
    def discretize_state(self, state):
        """Convert continuous state to a discretized key with appropriate granularity."""
        # Increase discretization precision for better state representation
        return tuple(np.round(state, decimals=2))
    
    def action_to_idx(self, action):
        """Convert action tuple to flat index."""
        pv_idx = self.action_space_pv.index(action[0])
        wt_idx = self.action_space_wt.index(action[1])
        grid_idx = self.action_space_grid.index(action[2])
        return pv_idx * (self.wt_size * self.grid_size) + wt_idx * self.grid_size + grid_idx
    
    def idx_to_action(self, idx):
        """Convert flat index to action tuple."""
        grid_idx = idx % self.grid_size
        temp = idx // self.grid_size
        wt_idx = temp % self.wt_size
        pv_idx = temp // self.wt_size
        
        return (self.action_space_pv[pv_idx],
                self.action_space_wt[wt_idx],
                self.action_space_grid[grid_idx])
    
    def estimate_total_energy(self, action, state):
        """Estimate the total energy generated by the given action."""
        pv_count, wt_count, grid_power = action
        
        pv_power = state[0]  #power factor
        wind_power = state[1]  # Wind power factor
        
        # Simplified energy calculations
        pv_energy = pv_count * pv_power
        wt_energy = wt_count * wind_power
        
        total_energy = pv_energy + wt_energy + grid_power
        return total_energy
        
    def get_valid_action(self, action, state):
        """Adjust the action to ensure total energy doesn't exceed the maximum limit."""
        pv_count, wt_count, grid_power = action
        total_energy = self.estimate_total_energy(action, state)
        
        if total_energy <= self.max_energy:
            return action  # Action is already valid
        
        # We need to reduce energy - prioritize reducing grid power first
        excess_energy = total_energy - self.max_energy
        adjusted_grid = max(0, grid_power - excess_energy)
        
        # Find the closest valid grid power value in our discretized space
        adjusted_grid = min(self.action_space_grid, key=lambda x: abs(x - adjusted_grid))
        
        adjusted_action = (pv_count, wt_count, adjusted_grid)
        
        # Verify the adjustment worked
        if self.estimate_total_energy(adjusted_action, state) > self.max_energy:
            # If still over limit, find the most conservative valid action
            for g in self.action_space_grid:
                test_action = (pv_count, wt_count, g)
                if self.estimate_total_energy(test_action, state) <= self.max_energy:
                    return test_action
            
            # If no valid action found with current PV/WT, reduce them
            return (0, 0, min(self.action_space_grid, key=lambda x: x if x <= self.max_energy else float('inf')))
        
        return adjusted_action
    
    def choose_action(self, state):
        """Epsilon-greedy policy for action selection with energy constraint."""
        state = self.discretize_state(state)
        
        # Explore: Randomly sample from action space components
        if np.random.rand() < self.epsilon:
            pv = random.choice(self.action_space_pv)
            wt = random.choice(self.action_space_wt)
            grid = random.choice(self.action_space_grid)
            action = (pv, wt, grid)
            return self.get_valid_action(action, state)  # Ensure action meets energy constraint
        
        # Exploit: Use the best known action
        if state not in self.q_table:
            # Initialize on-demand with a single random action
            self.q_table[state] = {0: 0.0}
            action = self.idx_to_action(0)
            return self.get_valid_action(action, state)  # Ensure action meets energy constraint
        
        # Find best action index
        best_idx = max(self.q_table[state], key=self.q_table[state].get)
        action = self.idx_to_action(best_idx)
        return self.get_valid_action(action, state)  # Ensure action meets energy constraint
    
    def update_target_network(self):
        """Update the target network with the current Q-table values."""
        self.target_q_table = {state: dict(values) for state, values in self.q_table.items()}
    
    def update_q_table(self, state, action, reward, cost, co2, next_state):
        """Update Q-table using the Q-learning formula with sparse representation."""
        state = self.discretize_state(state)
        next_state = self.discretize_state(next_state)
        
        # Convert action to index - handle non-discretized values
        try:
            pv = min(self.action_space_pv, key=lambda x: abs(x - action[0]))
            wt = min(self.action_space_wt, key=lambda x: abs(x - action[1]))
            grid = min(self.action_space_grid, key=lambda x: abs(x - action[2]))
            mapped_action = (pv, wt, grid)
            action_idx = self.action_to_idx(mapped_action)
        except ValueError:
            # Fallback if conversion fails
            action_idx = 0
        
        # Initialize state entry if new
        if state not in self.q_table:
            self.q_table[state] = {}
        
        # Initialize action value if new
        if action_idx not in self.q_table[state]:
            self.q_table[state][action_idx] = 0.0
        
        # Add to visited states
        self.visited_states.add(state)
        
        # Find maximum Q-value for next state using target network
        if next_state in self.target_q_table and self.target_q_table[next_state]:
            best_next_q = max(self.target_q_table[next_state].values())
        else:
            best_next_q = 0.0
        
        # Update rule
        self.q_table[state][action_idx] += self.alpha * (
            reward + self.gamma * best_next_q - self.q_table[state][action_idx]
        )
        
        # Remove near-zero Q-values to save memory
        if abs(self.q_table[state][action_idx]) < 1e-6:
            del self.q_table[state][action_idx]
            # Remove empty state entries
            if not self.q_table[state]:
                del self.q_table[state]
    
    def train(self, episodes=2000, max_steps=15000, cost_weight=0.5, co2_weight=0.5):
        """Train the agent with separate tracking of cost and CO2 metrics."""
        rewards = []
        cost_history = []
        co2_history = []
        energy_violations = []
        
        # Initialize target network
        self.update_target_network()
        
        for episode in range(episodes):
            state = self.env.reset()
            total_reward = 0
            episode_violations = 0
            episode_cost = 0
            episode_co2 = 0
            
            for step in range(max_steps):
                action = self.choose_action(state)
                
                # Check if original action would violate energy constraint
                total_energy = self.estimate_total_energy(action, state)
                if total_energy > self.max_energy:
                    episode_violations += 1
                
                # Calculate individual reward components
                cost = env.calculate_cost(action, state)
                co2 = env.calculate_co2(action, state)
                
                # Combined reward with weights
                reward = (env.cost_weight * cost) + (env.co2_weight * co2)
                
                # Additional penalty for actions that would exceed energy limit
                if total_energy > self.max_energy:
                    energy_penalty = -abs(total_energy - self.max_energy) * 0.1  # Scale penalty by excess
                    reward += energy_penalty
                
                next_state, env_reward, done, _ = self.env.step(action)
                
                # Update Q-table with our calculated reward
                self.update_q_table(state, action, reward, cost, co2, next_state)
                
                state = next_state
                total_reward += reward
                episode_cost += cost
                episode_co2 += co2
                
                if done:
                    break
            
            # Update target network periodically
            self.update_target_counter += 1
            if self.update_target_counter % self.target_update_frequency == 0:
                self.update_target_network()
            
            # Decay epsilon faster than before
            self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)
            
            # Record metrics
            rewards.append(total_reward)
            cost_history.append(episode_cost)
            co2_history.append(episode_co2)
            energy_violations.append(episode_violations)
            
            # Print progress less frequently
            if episode % 20 == 0:
                print(f"Episode {episode}, Reward: {total_reward:.2f}, Cost: {episode_cost:.2f}, CO2: {episode_co2:.2f}, Violations: {episode_violations}")
        
        self.cost_history = cost_history
        self.co2_history = co2_history
        self.energy_violation_history = energy_violations
        
        print(f"Training complete. Final epsilon: {self.epsilon:.4f}")
        print(f"Final Q-table size: {len(self.q_table)} states")
        
        return rewards, cost_history, co2_history, energy_violations